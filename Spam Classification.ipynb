{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f57538b",
   "metadata": {},
   "source": [
    "# TEXT CLASSIFICATION\n",
    "\n",
    "### **This is a project on Natural Language Processessing for text classification in Python using NLTK and Sci-kit-Learn .Here as the name suggest we will be classifying sms/text messages as either spam or not spam.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdf6f6c",
   "metadata": {},
   "source": [
    "### 1. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84b67a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import sklearn \n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16941323",
   "metadata": {},
   "source": [
    "### 2. Loading  the Dataset\n",
    "\n",
    "### **Now that we have ensured that our libraries are imported correctly, let's load the data set as a Pandas DataFrame. Furthermore, let's extract some useful information such as the column information and class distribution.**\n",
    "\n",
    "### **The data set we will be using comes from the UCI Machine Learning Repository.  It contains over 5000 SMS labeled messages that have been collected for mobile phone spam research. It can be downloaded from the following URL:**\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/sms+spam+collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0830155d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0                                                  1\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham               Will ü b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## loading the dataset of SMS messages\n",
    "\n",
    "Text_Data = pd.read_table('SMSSPamCollection', header=None, encoding='utf-8')\n",
    "\n",
    "Text_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b823c740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   0       5572 non-null   object\n",
      " 1   1       5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n"
     ]
    }
   ],
   "source": [
    "## print useful information about the dataset\n",
    "\n",
    "Text_Data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2f45185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## checking the class distribution\n",
    "\n",
    "Classes = Text_Data[0]\n",
    "\n",
    "Classes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d524e99",
   "metadata": {},
   "source": [
    "## 2. Preprocessing the Data\n",
    "\n",
    "### **Preprocessing the Text data is an essential step in natural language process. In the following cells, we will convert our class labels to binary values using the LabelEncoder from sklearn.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2c38e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "##  Converting the class labels to binary values, 0 = ham and 1 = spam\n",
    "\n",
    "Encoder = LabelEncoder()\n",
    "\n",
    "Binary_Labels = Encoder.fit_transform(Classes)\n",
    "\n",
    "Binary_Labels[:20] ## The first 20 instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11960066",
   "metadata": {},
   "source": [
    "## 2.1 Regular Expressions\n",
    "\n",
    "###  **Using regular expressions  we will be  replacing email addresses, URLs, phone numbers, other numbers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad46b75e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sayan\\AppData\\Local\\Temp/ipykernel_21388/1238977055.py:6: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  Email_replaced = Text_Messages.str.replace(r'^.+@[^\\.].*\\.[a-z]{2,}$','emailaddr')\n",
      "C:\\Users\\sayan\\AppData\\Local\\Temp/ipykernel_21388/1238977055.py:11: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  Urls_replaced = Email_replaced.str.replace(r'^http\\://[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(/\\S*)?$','webaddr')\n",
      "C:\\Users\\sayan\\AppData\\Local\\Temp/ipykernel_21388/1238977055.py:16: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  Moneysymb_replaced = Urls_replaced.str.replace(r'£|\\$', 'moneysymb')\n",
      "C:\\Users\\sayan\\AppData\\Local\\Temp/ipykernel_21388/1238977055.py:21: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  phnum_replaced = Moneysymb_replaced.str.replace(r'^\\(?[\\d]{3}\\)?[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}$','phonenumbr')\n",
      "C:\\Users\\sayan\\AppData\\Local\\Temp/ipykernel_21388/1238977055.py:26: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  Num_replaced = phnum_replaced.str.replace(r'\\d+(\\.\\d+)?', 'numbr')\n",
      "C:\\Users\\sayan\\AppData\\Local\\Temp/ipykernel_21388/1238977055.py:31: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  Punc_Removed = Num_replaced.str.replace(r'[^\\w\\d\\s]', ' ')\n",
      "C:\\Users\\sayan\\AppData\\Local\\Temp/ipykernel_21388/1238977055.py:36: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  Preprocessed = Punc_Removed.str.replace(r'\\s+', ' ')\n",
      "C:\\Users\\sayan\\AppData\\Local\\Temp/ipykernel_21388/1238977055.py:41: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  Preprocessed = Preprocessed.str.replace(r'^\\s+|\\s+?$', '')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       go until jurong point crazy available only in ...\n",
       "1                                 ok lar joking wif u oni\n",
       "2       free entry in numbr a wkly comp to win fa cup ...\n",
       "3             u dun say so early hor u c already then say\n",
       "4       nah i don t think he goes to usf he lives arou...\n",
       "                              ...                        \n",
       "5567    this is the numbrnd time we have tried numbr c...\n",
       "5568                  will ü b going to esplanade fr home\n",
       "5569    pity was in mood for that so any other suggest...\n",
       "5570    the guy did some bitching but i acted like i d...\n",
       "5571                            rofl its true to its name\n",
       "Name: 1, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Text_Messages = Text_Data[1]\n",
    "\n",
    "\n",
    "## 1. Replacing email addresses with 'emailaddr'\n",
    "\n",
    "Email_replaced = Text_Messages.str.replace(r'^.+@[^\\.].*\\.[a-z]{2,}$','emailaddr')\n",
    "\n",
    "\n",
    "## 2. Replacing URLs with 'webaddr'\n",
    "\n",
    "Urls_replaced = Email_replaced.str.replace(r'^http\\://[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(/\\S*)?$','webaddr')\n",
    "\n",
    "\n",
    "## 3. Replacing money symbols with 'moneysymb'\n",
    "\n",
    "Moneysymb_replaced = Urls_replaced.str.replace(r'£|\\$', 'moneysymb')\n",
    "\n",
    "\n",
    "## 4. Replacing 10 digit phone numbers (formats include paranthesis, spaces, no spaces, dashes) with 'phonenumber'\n",
    "\n",
    "phnum_replaced = Moneysymb_replaced.str.replace(r'^\\(?[\\d]{3}\\)?[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}$','phonenumbr')\n",
    "\n",
    "\n",
    "## 5. Replacing numbers with 'numbr'\n",
    "\n",
    "Num_replaced = phnum_replaced.str.replace(r'\\d+(\\.\\d+)?', 'numbr')\n",
    "\n",
    "\n",
    "## 6. # Removing punctuation or to be specific replacing it with blank single space.\n",
    "\n",
    "Punc_Removed = Num_replaced.str.replace(r'[^\\w\\d\\s]', ' ')\n",
    "\n",
    "\n",
    "## 7. Replacing whitespace between terms with a blank single space\n",
    "\n",
    "Preprocessed = Punc_Removed.str.replace(r'\\s+', ' ')\n",
    "\n",
    "\n",
    "## 8. Removing leading and trailing whitespace\n",
    "\n",
    "Preprocessed = Preprocessed.str.replace(r'^\\s+|\\s+?$', '')\n",
    "\n",
    "\n",
    "## 9. Changing every words to its lower case - Hello, HELLO, hello are all the same word\n",
    "\n",
    "Preprocessed = Preprocessed.str.lower()\n",
    "\n",
    "\n",
    "Preprocessed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1a42e3",
   "metadata": {},
   "source": [
    "###  **After this we will be removing the stopwords and word stems from the Text messages . Using corpus and  Porter Stemmer from nltk library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d6487e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       go jurong point crazy available bugis n great ...\n",
       "1                                 ok lar joking wif u oni\n",
       "2       free entry numbr wkly comp win fa cup final tk...\n",
       "3                     u dun say early hor u c already say\n",
       "4                  nah think goes usf lives around though\n",
       "                              ...                        \n",
       "5567    numbrnd time tried numbr contact u u moneysymb...\n",
       "5568                          ü b going esplanade fr home\n",
       "5569                                pity mood suggestions\n",
       "5570    guy bitching acted like interested buying some...\n",
       "5571                                       rofl true name\n",
       "Name: 1, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "## Removing stop words from text messages\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "Stopwords_removed = Preprocessed.apply(lambda x: ' '.join(term for term in x.split() if term not in stop_words))\n",
    "## Here we will have an empty string and for each text messages we are going to append to this string\n",
    "## all the words as long as they are not in the stop_words set that we have imported from the corpus \n",
    "\n",
    "Stopwords_removed ## Stop words has been removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35416524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       go jurong point crazi avail bugi n great world...\n",
       "1                                   ok lar joke wif u oni\n",
       "2       free entri numbr wkli comp win fa cup final tk...\n",
       "3                     u dun say earli hor u c alreadi say\n",
       "4                    nah think goe usf live around though\n",
       "                              ...                        \n",
       "5567    numbrnd time tri numbr contact u u moneysymbnu...\n",
       "5568                              ü b go esplanad fr home\n",
       "5569                                    piti mood suggest\n",
       "5570    guy bitch act like interest buy someth els nex...\n",
       "5571                                       rofl true name\n",
       "Name: 1, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing the word stems using a Porter stemmer\n",
    "\n",
    "Stemmer = nltk.PorterStemmer()\n",
    "\n",
    "Wordstem_removed = Stopwords_removed.apply(lambda x: ' '.join(Stemmer.stem(term) for term in x.split()))\n",
    "\n",
    "\n",
    "Wordstem_removed  ## Word Stems has been removed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca778d2",
   "metadata": {},
   "source": [
    "## 3. Generating Features\n",
    "\n",
    "### **Basically this a part of feature engineering .So what we will do is the words in each text message will be our features. For this purpose, it will be necessary to tokenize each word. We will use the 2000 most common words as features.( I chose 2000 words there is  no restriction you can choose any number within range. Mainly because more words means more features more time to train)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a52d3019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'numbr': 2648, 'u': 1207, 'call': 674, 'go': 456, 'get': 451, 'ur': 391, 'gt': 318, 'lt': 316, 'come': 304, 'moneysymbnumbr': 303, ...})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "Preprocessed = Wordstem_removed\n",
    "\n",
    "# creating bag-of-words\n",
    "\n",
    "All_words = [] ## empty list\n",
    "\n",
    "\n",
    "for message in Preprocessed:\n",
    "    \n",
    "    Words = word_tokenize(message)\n",
    "    \n",
    "    for word in Words:\n",
    "        \n",
    "        All_words.append(word) ## Appending each word to the empty string All_words\n",
    "        \n",
    "\n",
    "All_words = nltk.FreqDist(All_words) ## We are a frequency distribution of All words. \n",
    "                                     ## Means how many times each word has been repeated.\n",
    "\n",
    "All_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6e6a912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 6579\n",
      "Most common words: [('numbr', 2648), ('u', 1207), ('call', 674), ('go', 456), ('get', 451), ('ur', 391), ('gt', 318), ('lt', 316), ('come', 304), ('moneysymbnumbr', 303)]\n"
     ]
    }
   ],
   "source": [
    "# Let us print the total number of words and the 10 most common words.\n",
    "\n",
    "print('Number of words: {}'.format(len(All_words)))\n",
    "\n",
    "print('Most common words: {}'.format(All_words.most_common(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "357d1a79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go',\n",
       " 'jurong',\n",
       " 'point',\n",
       " 'crazi',\n",
       " 'avail',\n",
       " 'bugi',\n",
       " 'n',\n",
       " 'great',\n",
       " 'world',\n",
       " 'la']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the 2000 most common words as features\n",
    "\n",
    "Word_features = list(All_words.keys())[:2000]\n",
    "\n",
    "Word_features[:10] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fef9f1b",
   "metadata": {},
   "source": [
    "###  **After this we will define a function called find_features function that will determine which of this 2000 word features are contained in each review**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5d527ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The find_features function will determine which of the 1500 word features are contained in the review\n",
    "\n",
    "def find_features(message):\n",
    "    \n",
    "    Words = word_tokenize(message)\n",
    "    \n",
    "    Features = {}\n",
    "    \n",
    "    for word in Word_features:\n",
    "        \n",
    "        Features[word] = (word in Words)\n",
    "    \n",
    "    return Features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0061358f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n",
      "jurong\n",
      "point\n",
      "crazi\n",
      "avail\n",
      "bugi\n",
      "n\n",
      "great\n",
      "world\n",
      "la\n",
      "e\n",
      "buffet\n",
      "cine\n",
      "got\n",
      "amor\n",
      "wat\n"
     ]
    }
   ],
   "source": [
    "# Lets see an example!\n",
    "\n",
    "features = find_features(Preprocessed[0])\n",
    "\n",
    "for key,value in features.items():\n",
    "    if value == True:\n",
    "        print(key)\n",
    "        \n",
    "## Works fine..!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9728f437",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now lets to it for all the Messages\n",
    "\n",
    "Messages = list(zip(Preprocessed,Binary_Labels))\n",
    "\n",
    "\n",
    "## defining a seed for reproducibility\n",
    "\n",
    "seed = 1\n",
    "\n",
    "np.random.seed = seed\n",
    "\n",
    "np.random.shuffle(Messages)\n",
    "\n",
    "\n",
    "##  Calling the  find_features function for each SMS message\n",
    "\n",
    "Feature_sets = [(find_features(text), label) for (text, label) in Messages]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7d6fe4",
   "metadata": {},
   "source": [
    "### Now we will split the Feature_sets into training and testing datasets using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "210d9cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# splitting the data into training and testing datasets\n",
    "\n",
    "Training,Testing = train_test_split(Feature_sets, test_size = 0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a74f14f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4179\n",
      "1393\n"
     ]
    }
   ],
   "source": [
    "print(len(Training))\n",
    "print(len(Testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7423275",
   "metadata": {},
   "source": [
    "### 4. Scikit-Learn Classifiers with NLTK\n",
    "\n",
    "Now that we have our training dataset, We'll need to import each algorithm we plan on using from sklearn.  We also need to import some performance metrics, such as accuracy_score and classification_report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cee2b81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.linear_model import RidgeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c0af99e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Nearest Neighbors Accuracy: 92.96482412060301\n",
      "Decision Tree Accuracy: 97.20028715003589\n",
      "Random Forest Accuracy: 98.85139985642498\n",
      "Logistic Regression Accuracy: 98.63603732950466\n",
      "SGD Classifier Accuracy: 98.34888729361091\n",
      "Naive Bayes Accuracy: 97.77458722182341\n",
      "SVM Linear Accuracy: 98.7078248384781\n",
      "AdaBoost Classifier Accuracy: 98.06173725771716\n",
      "Gradient Boosting Classifier Accuracy: 98.34888729361091\n",
      "Bagging Classifier Accuracy: 98.1335247666906\n",
      "Extra Trees Classifier Accuracy: 98.92318736539842\n",
      "Ridge Classifier Accuracy: 97.4156496769562\n"
     ]
    }
   ],
   "source": [
    "# Defining each models to train\n",
    "\n",
    "names = [\"K Nearest Neighbors\", \"Decision Tree\", \"Random Forest\", \"Logistic Regression\", \"SGD Classifier\",\n",
    "         \"Naive Bayes\", \"SVM Linear\",\"AdaBoost Classifier\",\"Gradient Boosting Classifier\",\"Bagging Classifier\",\"Extra Trees Classifier\",\n",
    "         \"Ridge Classifier\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    LogisticRegression(),\n",
    "    SGDClassifier(max_iter = 100),\n",
    "    MultinomialNB(),\n",
    "    SVC(kernel = 'linear'),\n",
    "    AdaBoostClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    BaggingClassifier(),\n",
    "    ExtraTreesClassifier(),\n",
    "    RidgeClassifier(max_iter=100)]\n",
    "\n",
    "\n",
    "models = zip(names, classifiers)\n",
    "\n",
    "\n",
    "for name, model in models:\n",
    "    \n",
    "    nltk_model = SklearnClassifier(model)\n",
    "    \n",
    "    nltk_model.train(Training)\n",
    "    \n",
    "    accuracy = nltk.classify.accuracy(nltk_model, Testing)*100\n",
    "    \n",
    "    print(\"{} Accuracy: {}\".format(name, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276250c9",
   "metadata": {},
   "source": [
    "## Ensemble methods - Voting classifier (Combination of all the classifier methods into a single classification algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "23eccdb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier: Accuracy: 97.4156496769562\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "names2 = [\"K Nearest Neighbors\", \"Decision Tree\", \"Random Forest\", \"Logistic Regression\", \"SGD Classifier\",\n",
    "         \"Naive Bayes\", \"SVM Linear\",\"AdaBoost Classifier\",\"Gradient Boosting Classifier\",\"Bagging Classifier\",\"Extra Trees Classifier\",\n",
    "         \"Ridge Classifier\"]\n",
    "\n",
    "classifiers2 = [\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    LogisticRegression(),\n",
    "    SGDClassifier(max_iter = 100),\n",
    "    MultinomialNB(),\n",
    "    SVC(kernel = 'linear'),\n",
    "    AdaBoostClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    BaggingClassifier(),\n",
    "    ExtraTreesClassifier(),\n",
    "    RidgeClassifier(max_iter=100)]\n",
    "\n",
    "\n",
    "models2 = list(zip(names2,classifiers2))\n",
    "\n",
    "nltk_ensemble = SklearnClassifier(VotingClassifier(estimators = models2, voting = 'hard', n_jobs = -1))\n",
    "nltk_ensemble.train(Training)\n",
    "accuracy = nltk.classify.accuracy(nltk_model, Testing)*100\n",
    "print(\"Voting Classifier: Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "840ad1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make class label prediction for testing set\n",
    "\n",
    "txt_features, labels = zip(*Testing)\n",
    "\n",
    "prediction = nltk_ensemble.classify_many(txt_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0d3cdec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      1205\n",
      "           1       1.00      0.90      0.95       188\n",
      "\n",
      "    accuracy                           0.99      1393\n",
      "   macro avg       0.99      0.95      0.97      1393\n",
      "weighted avg       0.99      0.99      0.99      1393\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">actual</th>\n",
       "      <th>ham</th>\n",
       "      <td>1205</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>18</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            predicted     \n",
       "                  ham spam\n",
       "actual ham       1205    0\n",
       "       spam        18  170"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print a confusion matrix and a classification report\n",
    "\n",
    "print(classification_report(labels, prediction))\n",
    "\n",
    "pd.DataFrame(\n",
    "    confusion_matrix(labels, prediction),\n",
    "    index = [['actual', 'actual'], ['ham', 'spam']],\n",
    "    columns = [['predicted', 'predicted'], ['ham', 'spam']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21ef81c",
   "metadata": {},
   "source": [
    "### 0 is our ham class and 1 is our spam class ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7db40f",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
